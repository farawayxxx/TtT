# TtT
code for our ACL2021 paper "Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction"

The pretrained BERT model is here: 

Training:
```
./train.sh
```

Tips to reproduce the results:
- More epochs: more than 250;
- Larger batchsize on GPUs such as V100.
